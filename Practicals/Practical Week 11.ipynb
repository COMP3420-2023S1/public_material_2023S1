{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Week 11: Sequence-to-sequence models\n",
    "\n",
    "There are three important tasks to complete this week:\n",
    "\n",
    "1. Learning how to use LSTM and Transformers to process data sequences. You will find yourself waiting a long time\n",
    "   for many of these computations to complete. While you wait:\n",
    "\n",
    "2. Complete the course survey. It's in ilearn, and this link will show you how to view it and complete it:\n",
    "https://ishare.mq.edu.au/prod/items/912f85f6-70e8-48a5-94ca-d3928e9d7c3c/1/viewcontent/\n",
    "\n",
    "\n",
    "3. Sign up to get API access to OpenAI, which you will need for next week's practical. https://platform.openai.com/signup \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence to sequence\n",
    "\n",
    "We're going to do a task that even ChatGPT-4 doesn't get right: arithmetic on long integer sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell creates a list of example bitwise-or calculations using python's `|` operator.\n",
    "\n",
    "If you aren't familiar with Python's bit-handling functions, or what a bitwise-or does, here's a short\n",
    "introduction: https://realpython.com/python-bitwise-operators/#bitwise-logical-operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_facts = []\n",
    "for i in range(64):\n",
    "    for j in range(64):\n",
    "        k = i | j\n",
    "        math_facts.append(f\"{i:06b} | {j:06b} = {k:06b}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the random library (or any other way you like) to see a few samples from the `math_facts` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "We need to process these strings so that they can be used in a sequence-to-sequence model.\n",
    "\n",
    "- For each string, split it up on the equals sign.\n",
    "- On the left hand side of the equals sign, add a space between each digit so that each digit is its own token\n",
    "- On the right hand side of the equals sign, generate 9 training samples. Encode 0 as `tf.convert_to_tensor([1,0,0])`, 1 as `tf.convert_to_tensor([0,1,0])` and \".\" as `tf.convert_to_tensor([0,0,1])`\n",
    "\n",
    "e.g. suppose you were working with the statement:\n",
    "\n",
    "```001000 | 010101 = 011101.```\n",
    "\n",
    "Then you want to turn this into:\n",
    "\n",
    "```\n",
    "{'context': '0 0 1 0 0 0 | 0 1 0 1 0 1 =', 'next_token': '0'}, \n",
    "{'context': '0 0 1 0 0 0 | 0 1 0 1 0 1 = 0', 'next_token': '1'}, \n",
    "{'context': '0 0 1 0 0 0 | 0 1 0 1 0 1 = 0 1', 'next_token': '1'},\n",
    "{'context': '0 0 1 0 0 0 | 0 1 0 1 0 1 = 0 1 1', 'next_token': '1'},\n",
    "{'context': '0 0 1 0 0 0 | 0 1 0 1 0 1 = 0 1 1 1', 'next_token': '0'},\n",
    "{'context': '0 0 1 0 0 0 | 0 1 0 1 0 1 = 0 1 1 1 0', 'next_token': '1'},\n",
    "{'context': '0 0 1 0 0 0 | 0 1 0 1 0 1 = 0 1 1 1 0 1', 'next_token': '.'},\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "r = re.compile('(\\d)(\\d)(\\d)(\\d)(\\d)(\\d)'\n",
    "               ' \\| '\n",
    "               '(\\d)(\\d)(\\d)(\\d)(\\d)(\\d)'\n",
    "               ' = '\n",
    "               '(\\d)(\\d)(\\d)(\\d)(\\d)(\\d)\\.'\n",
    "              )\n",
    "data = []\n",
    "token_lookup = {'0': [1,0,0], '1': [0,1,0]}\n",
    "\n",
    "for fact in math_facts:\n",
    "    g = r.search(fact)\n",
    "    lhs = f\"{g[1]} {g[2]} {g[3]} {g[4]} {g[5]} {g[6]} | \"\n",
    "    lhs += f\"{g[7]} {g[8]} {g[9]} {g[10]} {g[11]} {g[12]} =\"\n",
    "    \n",
    "    data.append({'context': lhs, 'next_token': token_lookup[g[13]]} )\n",
    "    lhs += f\" {g[13]}\"\n",
    "    data.append({'context': lhs, 'next_token': token_lookup[g[14]]} )\n",
    "    lhs += f\" {g[14]}\"\n",
    "    data.append({'context': lhs, 'next_token': token_lookup[g[15]]} )\n",
    "    lhs += f\" {g[15]}\"\n",
    "    data.append({'context': lhs, 'next_token': token_lookup[g[16]]} )\n",
    "    lhs += f\" {g[16]}\"\n",
    "    data.append({'context': lhs, 'next_token': token_lookup[g[17]]} )\n",
    "    lhs += f\" {g[17]}\"\n",
    "    data.append({'context': lhs, 'next_token': token_lookup[g[18]]} )\n",
    "    lhs += f\" {g[18]}\"\n",
    "    #data.append({'context': lhs, 'next_token': tf.convert_to_tensor([0,0,1])} )\n",
    "    data.append({'context': lhs, 'next_token': [0,0,1]} )\n",
    "\n",
    "df = pd.DataFrame.from_records(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View a sample of the `df` dataframe to confirm it matches what you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the `df` dataframe up into train, validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `keras.layers.TextVectorization` object with the following constructor parameters:\n",
    "\n",
    "- split=\"whitespace\"\n",
    "- standardize=None\n",
    "- output_sequence_length=26 (the longest possible sequence)\n",
    "\n",
    "Adapt it to the training data (the `context` column) and vectorise the training, validation and test texts.\n",
    "\n",
    "(Bonus challenge: this is one of the few times where you can actually give the vectorizer a vocabulary, and\n",
    "you don't have to adapt from training data. Can you figure out how?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target values need to be converted into tensorflow Tensors. Unfortunately, `tf.convert_to_tensor` doesn't\n",
    "handle pandas Series correctly. Use this work-around:\n",
    "\n",
    "`train_y = tf.convert_to_tensor(list(train.next_token))`\n",
    "\n",
    "(Create `train_y`, `validation_y` and `test_y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model with:\n",
    "\n",
    "- an input layer\n",
    "\n",
    "- an embedding layer (a 4-dimensional embedding space will be sufficient)\n",
    "\n",
    "- a bidirectional LSTM layer with 16 neurons in it\n",
    "\n",
    "- an output layer of 3 dense neurons with a softmax activation\n",
    "\n",
    "Compile it, and use a loss function of `categorical_crossentropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model using the training data, add validation data and early stopping. You'll find that it \n",
    "reaches 100% accuracy after about 100-120 epochs.\n",
    "\n",
    "Surprisingly, it doesn't ever overfit. Make a graph of it if you want to (or just watch the numbers\n",
    "fly past as it trains).\n",
    "\n",
    "While you are waiting for this to complete, it would be a great time to **do the student evaluation survey now**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student evaluation survey\n",
    "\n",
    "Set this value to true when you have done your student evaluation survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_have_completed_my_student_evaluation_survey = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI API key\n",
    "\n",
    "Set this value to true when you have signed up for your API key.\n",
    "\n",
    "Remember that the API key can only be viewed once, so save it somewhere safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_have_signed_up_for_openai = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the sequence-to-sequence model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the test accuracy. It should be very good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that takes a bitwise-or arithmetic problem, an optional list of already-returned tokens and returns the next token.\n",
    "\n",
    "e.g. \n",
    "`get_next_token('1 0 0 1 0 0 | 0 0 0 0 0 1', '1 0 0')` should return \"1\".\n",
    "\n",
    "You will need to:\n",
    "\n",
    "- concatentate the first and second arguments (with an equals sign (=) between them\n",
    "\n",
    "- vectorize that input string\n",
    "\n",
    "- use the model to predict the probabilities of the three options for the next token (0,1 or .)\n",
    "\n",
    "- find the option that has the highest probability\n",
    "\n",
    "- return that as a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it out with a sample problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a function `get_full_answer` that calls `get_next_token` repeatedly with more context each time\n",
    "until it gets a `.`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out with a sample problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers (Optional)\n",
    "\n",
    "Depending on how fast your computer is, how long it takes to do the student survey and so on, you \n",
    "might or might not have a lot of time left over.\n",
    "\n",
    "If you have run through all the above very quickly, go through the\n",
    "https://www.tensorflow.org/text/tutorials/transformer\n",
    "\n",
    "The tutorial is slightly out-of-date (as almost everything involving transformers is!)\n",
    "because it re-implements components that already exist in the `keras_nlp` library, but it\n",
    "is otherwise excellent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
