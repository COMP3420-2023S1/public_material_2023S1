{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet\n",
    "\n",
    "This first part should be able to run on almost any system. No need for Google Colab for this part!\n",
    "\n",
    "Import the WordNet library within NLTK's corpus library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word \"grade\" can mean many things in English. Use `wn.synsets()` to find the different synsets of these.\n",
    "\n",
    "Print out the definitions for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the synset `mark.n.01` and print out some example usages of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it have any hyponyms? (Examples / instances of it... `A ____ is a mark.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are its hypernyms? `A mark is a ____`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What lemma names does `mark` have? i.e. What are some synonyms of `mark`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSQVr3b7v_Rg"
   },
   "source": [
    "## An End-to-End Text Classification System\n",
    "\n",
    "This is a combination of a review of last week (making a classifier) and also practice with embeddings.\n",
    "\n",
    "The task will be to classify questions.\n",
    "\n",
    "It's gong to be heavy so to run this task we advice that you use [Google Colaboratory](https://colab.research.google.com) (also called Google Colab), which is a cloud solution to run Jupyter notebooks. The demonstrator will show how to use Google Colab. For additional information and to practice with the use of notebooks in Google Colab, you can also follow this link:\n",
    "\n",
    "* [Welcome notebook and link to additional resources](https://colab.research.google.com/notebooks/welcome.ipynb)\n",
    "\n",
    "### Question Classification\n",
    "\n",
    "NLTK has a corpus of questions and their question types according to a particular classification scheme (e.g. DESC refers to a question expecting a descriptive answer, such as one starting with \"How\"; HUM refers to a question expecting an answer referring to a human). Below is an example of use of the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HdoJ387Vv_Rq",
    "outputId": "6043753b-a65e-490e-91a1-3d33cf00cc9e"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"qc\")\n",
    "from nltk.corpus import qc\n",
    "train = qc.tuples(\"train.txt\")\n",
    "test = qc.tuples(\"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U78gbdkzv_SM",
    "outputId": "5791538d-7b76-48cf-f652-94b88702e39a"
   },
   "outputs": [],
   "source": [
    "train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dUCCU6awv_Si",
    "outputId": "d5640021-57a0-4ac6-847c-45a18c41b0ef"
   },
   "outputs": [],
   "source": [
    "test[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCOTaE2Yv_S-"
   },
   "source": [
    "### Exercise: Find all question types\n",
    "Write Python code that lists all the possible question types of the training set (**remember: for data exploration, never look at the test set**).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yn7EUUwUv_TE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AvETLOhxv_TW",
    "outputId": "cad1c622-aacb-493b-f321-c69d994deeca"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inQh0Nrjv_Tp"
   },
   "source": [
    "### Exercise: Find all general types\n",
    "\n",
    "The question types have two parts. The first part describes a general type, and the second part defines a subtype. For example, the question type `DESC:manner` belongs to the general `DESC` type and within that type to the `manner` subtype. Let's focus on the general types only. Write Python code that lists all the possible general types (there are 6 of them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C4pjpWV9v_Tt",
    "outputId": "f306a984-6e4d-4410-e2a6-79d7e6a2cbcc"
   },
   "outputs": [],
   "source": [
    "general_types = list(set([q.split(':')[0] for q in qtypes]))\n",
    "general_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnMXsRBMv_T-"
   },
   "source": [
    "### Exercise: Partition the data\n",
    "There is a train and test data, but for this exercise we want to have a partition into train, dev-test, and test. In this exercise, combine all data into one array and do a 3-way partition into train, dev-test, and test. Make sure that you shuffle the data prior to doing the partition. Also, make sure that you only use the general label types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qq6JJ-EBv_UE",
    "outputId": "6fac9fe8-b219-4812-dad9-85035dff7325"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWQx8J2tv_Ua"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iri6AvfYv_Uj"
   },
   "source": [
    "### Exercise: Tokenise the data\n",
    "\n",
    "Use Keras' tokeniser to tokenise all the data. For this exercise we will use only the 100 most frequent words in the training set (since you aren't supposed to use the dev-test or test sets to extract features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Jbf0L_Qv_Un"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UE2ecEF2v_U2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2gc3ClcGwCyz",
    "outputId": "10ac4021-da08-481e-c076-1401646e1c97"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tQqQC4d3wHQd",
    "outputId": "e9f722e2-9da4-4ea7-da30-1b8bf9d403e2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "lxMDEFJuxfiz",
    "outputId": "c8b2922b-8404-412d-dfb2-2d326daf6534"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "Zob47UVKxfm0",
    "outputId": "69137d30-4a55-4543-eae2-b5b1d6b4eee2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epuXbJoyv_VA"
   },
   "source": [
    "### Exercise: Vectorize the data\n",
    "The following code shows the distribution of lengths of my training data (could be different in your training data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "ZEaD0ouFv_VE",
    "outputId": "1ee82ebd-91c9-417e-eff9-3ce784e4276d"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.hist([len(d) for d in indices_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t34OuB9vv_VQ"
   },
   "source": [
    "The histogram shows that the longest question in the training data has 30 word indices, but by far most of the questions have at least 20. Based on this, use Keras' `pad_sequences` to vectorize the questions into sequences of 20 word indices. The default will be to truncate the beginning, but we want to truncate the end (since the first words of a question are often very important to determine the question type). For this you can use the option `truncating='post'`: https://keras.io/preprocessing/sequence/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15jgaUwpv_VS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qn1vV1Kov_Vd"
   },
   "source": [
    "### Exercise: Vectorise the labels\n",
    "Convert the labels to one-hot encoding. If you use Keras' `to_categorical`, you will first need to convert the labels to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OfRBWqadv_Vh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwNCab0Kv_Vs"
   },
   "source": [
    "### Exercise: Define the model\n",
    "\n",
    "Define a model for classification. For this model, use a feedforward architecture with an embedding layer of size 20, a layer that computes the average of word embeddings (use `GlobalAveragePooling1D`), a hidden layer of 16 units, and `relu` activation. You need to determine the size and activation of the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pBcT2-btv_Vu",
    "outputId": "22098ab0-dbe5-4a2d-fe0e-c455d571532a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQxpGsUcv_V5"
   },
   "source": [
    "### Exercise: Train and evaluate\n",
    "Train your model. In the process you need to determine the optimal number of epochs. Then answer the following questions:\n",
    "1. What was the optimal number of epochs and how did you determine this?\n",
    "2. Is the system overfitting? Justify your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8DfIKpyYv_V7",
    "outputId": "a88c6ac2-3174-44fd-c482-e1ef5f5abafc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "yaVdjCqEv_WD",
    "outputId": "487917fd-53ed-403e-98c8-6733ea181a47"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LKu0AEYCv_WN"
   },
   "source": [
    "Based on the validation loss, a good value of epochs is 41. At this point the system is overfitting already but the validation loss appears to be optimal. Let's check with the accuracy as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "1rDfPPOxv_WP",
    "outputId": "7a08cde7-5b3f-43df-eb45-450221653783"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_M6drwCv_WY"
   },
   "source": [
    "Yes accuracy looks near optimal at 41 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbK8NkhRv_Wb"
   },
   "source": [
    "### Optional Exercise: Data exploration\n",
    "Plot the distribution of labels in the training data and compare with the distribution of labels in the devtest data. Plot also the distribution of predictions in the devtest data. What can you learn from this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZYWeFIznv_Wg",
    "outputId": "e1a6df13-88b3-4d2a-9dc0-b46fa34df8a3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "N56xRMjiv_Wn",
    "outputId": "ec108cb6-bfe4-45a0-b098-3cc2193f51bb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ONeUnyw9v_Wv",
    "outputId": "69894ddf-958d-4d88-f29d-885058e74fdc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "z7V9rm_lv_W3",
    "outputId": "9a9d4630-cd55-43f0-cc4a-04f0f0966521"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USV5vlpNv_XB"
   },
   "source": [
    "The training and devtest sets have similar distributions. The data is somewhat balanced except for the `ABBR` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D7PTmPIFv_XD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nbW2nOh7v_XJ",
    "outputId": "12815044-3710-4f4f-8c1c-8bd74502de87"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "qVxL8ADAv_XV",
    "outputId": "69cb9608-a0e3-4be9-830b-44016693ca38"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GA8W2g1v_Xf"
   },
   "source": [
    "The predicted labels have a different distribution to the labels of the training and devtest data, and there are no predictions for the ABBR class. This is a common issue. Sometimes, the system does not learn classes that have poor representation in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySiHuaA450dL"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOUTCrdqv_Xi"
   },
   "source": [
    "### Optional Exercise: Improve your system\n",
    "\n",
    "Try the following options:\n",
    "\n",
    "1. Use pre-trained word embeddings\n",
    "2. Use recurrent neural networks.\n",
    "\n",
    "Feel free to try each option separately and in combination, and compare the results. Feel also free to try with other variants of the initial architecture, such as:\n",
    "\n",
    "1. Introducing more hidden layers.\n",
    "2. Changing the size of embeddings.\n",
    "3. Changing the number of units in the hidden layer(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Q-1GtYOv_Xl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "W05_Practical_Solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
